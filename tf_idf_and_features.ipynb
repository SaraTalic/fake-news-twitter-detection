{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428bb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc02b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d92ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grupa 1: Pozitivne emocije\n",
    "positive_emotions = [\n",
    "    'cheerfulness', 'joy', 'contentment', 'love', 'warmth',\n",
    "    'positive_emotion', 'fun', 'giving', 'friends'\n",
    "]\n",
    "\n",
    "# Grupa 2: Negativne emocije\n",
    "negative_emotions = [\n",
    "    'sadness', 'disgust', 'suffering', 'negative_emotion',\n",
    "    'weakness', 'neglect'\n",
    "]\n",
    "\n",
    "# Grupa 3: Socijalne emocije\n",
    "social_emotions = [\n",
    "    'pride', 'shame', 'politeness', 'affection', 'leader',\n",
    "    'dominant_personality', 'childish', 'trust', 'sympathy'\n",
    "]\n",
    "\n",
    "# Grupa 4: Intenzivne emocije\n",
    "intense_emotions = [\n",
    "    'surprise', 'rage', 'horror', 'fear', 'exasperation',\n",
    "    'nervousness', 'irritability', 'torment', 'pain', 'hate', 'anger'\n",
    "]\n",
    "\n",
    "# Grupa 5: Kognitivno-emotivne emocije\n",
    "cognitive_emotions = [\n",
    "    'anticipation', 'confusion', 'envy', 'disappointment',\n",
    "    'optimism', 'zest', 'achievement'\n",
    "]\n",
    "\n",
    "# Kreiranje novih kolona kao zbir postojeÄ‡ih\n",
    "df['emotion_positive'] = df[[f'empath_result.{x}' for x in positive_emotions]].sum(axis=1)\n",
    "df['emotion_negative'] = df[[f'empath_result.{x}' for x in negative_emotions]].sum(axis=1)\n",
    "df['emotion_social']   = df[[f'empath_result.{x}' for x in social_emotions]].sum(axis=1)\n",
    "df['emotion_intense']  = df[[f'empath_result.{x}' for x in intense_emotions]].sum(axis=1)\n",
    "df['emotion_cognitive'] = df[[f'empath_result.{x}' for x in cognitive_emotions]].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa71d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_users = ['followers_count', 'favourites_count','friends_count',\n",
    "       'statuses_count', 'listed_count', 'cred','BotScore',\n",
    "       'normalize_influence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9143d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_empath = ['emotion_positive', 'emotion_negative',\n",
    "       'emotion_social', 'emotion_intense', 'emotion_cognitive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1857c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['tweet_tokens'] = df['tweet_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a90278",
   "metadata": {},
   "source": [
    "# TF-IDF + User/Empath Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d641d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_text = vectorizer.fit_transform(df['tweet_new_x'])\n",
    "\n",
    "X_users = df[feature_cols_users].values\n",
    "X_empath = df[feature_cols_empath].values\n",
    "X_users_empath = df[feature_cols_users + feature_cols_empath].values\n",
    "\n",
    "X_tfidf_users = hstack([X_text, X_users])\n",
    "X_tfidf_empath = hstack([X_text, X_empath])\n",
    "X_tfidf_users_empath = hstack([X_text, X_users_empath])\n",
    "\n",
    "y = df['BinaryNumTarget'].astype(int)\n",
    "\n",
    "X_train_users, X_test_users, y_train_users, y_test_users = train_test_split(X_tfidf_users, y, test_size=0.2, random_state=1)\n",
    "X_train_empath, X_test_empath, y_train_empath, y_test_empath = train_test_split(X_tfidf_empath, y, test_size=0.2, random_state=1)\n",
    "X_train_users_empath, X_test_users_empath, y_train_users_empath, y_test_users_empath = train_test_split(X_tfidf_users_empath, y, test_size=0.2, random_state=1)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, title):\n",
    "    print(f\"\\nRezultati za: {title}\")\n",
    "  \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(name)\n",
    "        report = classification_report(y_test, y_pred, digits=3)\n",
    "        print(report)\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f552e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: TF-IDF + User features\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.972     0.973     0.973     12990\n",
      "           1      0.975     0.974     0.974     13849\n",
      "\n",
      "    accuracy                          0.973     26839\n",
      "   macro avg      0.973     0.973     0.973     26839\n",
      "weighted avg      0.973     0.973     0.973     26839\n",
      "\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.958     0.959     0.959     12990\n",
      "           1      0.962     0.961     0.961     13849\n",
      "\n",
      "    accuracy                          0.960     26839\n",
      "   macro avg      0.960     0.960     0.960     26839\n",
      "weighted avg      0.960     0.960     0.960     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.940     0.926     0.933     12990\n",
      "           1      0.932     0.944     0.938     13849\n",
      "\n",
      "    accuracy                          0.936     26839\n",
      "   macro avg      0.936     0.935     0.936     26839\n",
      "weighted avg      0.936     0.936     0.936     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.686     0.682     0.684     12990\n",
      "           1      0.704     0.707     0.705     13849\n",
      "\n",
      "    accuracy                          0.695     26839\n",
      "   macro avg      0.695     0.695     0.695     26839\n",
      "weighted avg      0.695     0.695     0.695     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.949     0.879     0.912     12990\n",
      "           1      0.894     0.955     0.923     13849\n",
      "\n",
      "    accuracy                          0.918     26839\n",
      "   macro avg      0.921     0.917     0.918     26839\n",
      "weighted avg      0.920     0.918     0.918     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [11:16:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.945     0.961     12990\n",
      "           1      0.950     0.979     0.965     13849\n",
      "\n",
      "    accuracy                          0.963     26839\n",
      "   macro avg      0.964     0.962     0.963     26839\n",
      "weighted avg      0.963     0.963     0.963     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.971     0.974     12990\n",
      "           1      0.973     0.979     0.976     13849\n",
      "\n",
      "    accuracy                          0.975     26839\n",
      "   macro avg      0.975     0.975     0.975     26839\n",
      "weighted avg      0.975     0.975     0.975     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_users = train_and_evaluate(X_train_users, X_test_users, y_train_users, y_test_users, \"TF-IDF + User features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f08c8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: TF-IDF + Empath features\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.975     0.980     0.978     12990\n",
      "           1      0.981     0.977     0.979     13849\n",
      "\n",
      "    accuracy                          0.978     26839\n",
      "   macro avg      0.978     0.978     0.978     26839\n",
      "weighted avg      0.978     0.978     0.978     26839\n",
      "\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.968     0.964     0.966     12990\n",
      "           1      0.967     0.970     0.968     13849\n",
      "\n",
      "    accuracy                          0.967     26839\n",
      "   macro avg      0.967     0.967     0.967     26839\n",
      "weighted avg      0.967     0.967     0.967     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.953     0.933     0.943     12990\n",
      "           1      0.939     0.957     0.948     13849\n",
      "\n",
      "    accuracy                          0.945     26839\n",
      "   macro avg      0.946     0.945     0.945     26839\n",
      "weighted avg      0.945     0.945     0.945     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.998     0.443     0.614     12990\n",
      "           1      0.657     0.999     0.793     13849\n",
      "\n",
      "    accuracy                          0.730     26839\n",
      "   macro avg      0.828     0.721     0.703     26839\n",
      "weighted avg      0.822     0.730     0.706     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.991     0.986     0.989     12990\n",
      "           1      0.987     0.991     0.989     13849\n",
      "\n",
      "    accuracy                          0.989     26839\n",
      "   macro avg      0.989     0.989     0.989     26839\n",
      "weighted avg      0.989     0.989     0.989     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [13:32:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.945     0.961     12990\n",
      "           1      0.950     0.978     0.964     13849\n",
      "\n",
      "    accuracy                          0.962     26839\n",
      "   macro avg      0.963     0.962     0.962     26839\n",
      "weighted avg      0.963     0.962     0.962     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.971     0.973     12990\n",
      "           1      0.973     0.978     0.975     13849\n",
      "\n",
      "    accuracy                          0.974     26839\n",
      "   macro avg      0.974     0.974     0.974     26839\n",
      "weighted avg      0.974     0.974     0.974     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_empath = train_and_evaluate(X_train_empath, X_test_empath, y_train_empath, y_test_empath, \"TF-IDF + Empath features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719dc60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: TF-IDF + User + Empath features\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.970     0.972     0.971     12990\n",
      "           1      0.973     0.972     0.973     13849\n",
      "\n",
      "    accuracy                          0.972     26839\n",
      "   macro avg      0.972     0.972     0.972     26839\n",
      "weighted avg      0.972     0.972     0.972     26839\n",
      "\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.960     0.958     0.959     12990\n",
      "           1      0.960     0.962     0.961     13849\n",
      "\n",
      "    accuracy                          0.960     26839\n",
      "   macro avg      0.960     0.960     0.960     26839\n",
      "weighted avg      0.960     0.960     0.960     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.940     0.926     0.933     12990\n",
      "           1      0.932     0.944     0.938     13849\n",
      "\n",
      "    accuracy                          0.936     26839\n",
      "   macro avg      0.936     0.935     0.936     26839\n",
      "weighted avg      0.936     0.936     0.936     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.687     0.683     0.685     12990\n",
      "           1      0.705     0.709     0.707     13849\n",
      "\n",
      "    accuracy                          0.696     26839\n",
      "   macro avg      0.696     0.696     0.696     26839\n",
      "weighted avg      0.696     0.696     0.696     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.879     0.912     12990\n",
      "           1      0.894     0.954     0.923     13849\n",
      "\n",
      "    accuracy                          0.918     26839\n",
      "   macro avg      0.921     0.917     0.917     26839\n",
      "weighted avg      0.920     0.918     0.918     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [14:35:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.976     0.943     0.959     12990\n",
      "           1      0.949     0.978     0.963     13849\n",
      "\n",
      "    accuracy                          0.961     26839\n",
      "   macro avg      0.962     0.961     0.961     26839\n",
      "weighted avg      0.962     0.961     0.961     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.971     0.974     12990\n",
      "           1      0.973     0.979     0.976     13849\n",
      "\n",
      "    accuracy                          0.975     26839\n",
      "   macro avg      0.975     0.975     0.975     26839\n",
      "weighted avg      0.975     0.975     0.975     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_users_empath = train_and_evaluate(X_train_users_empath, X_test_users_empath, y_train_users_empath, y_test_users_empath, \"TF-IDF + User + Empath features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ec2b3",
   "metadata": {},
   "source": [
    "# Word2Vec + User/Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da825c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['tweet_tokens'].tolist()\n",
    "\n",
    "w2v_model = Word2Vec(sentences, vector_size=300, window=10, min_count=5, workers=4, sg=1, negative=10)\n",
    "\n",
    "def vectorize_tweet(tokens):\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "X_w2v = np.array([vectorize_tweet(tokens) for tokens in df['tweet_tokens']])\n",
    "X_users = df[feature_cols_users].values\n",
    "X_empath = df[feature_cols_empath].values\n",
    "\n",
    "X_w2v_users = np.hstack([X_w2v, X_users])\n",
    "X_w2v_empath = np.hstack([X_w2v, X_empath])\n",
    "X_w2v_users_empath = np.hstack([X_w2v, X_users, X_empath])\n",
    "\n",
    "y = df['BinaryNumTarget'].astype(int)\n",
    "\n",
    "X_train_w2v_users, X_test_w2v_users, y_train_w2v_users, y_test_w2v_users = train_test_split(X_w2v_users, y, test_size=0.2, random_state=1)\n",
    "X_train_w2v_empath, X_test_w2v_empath, y_train_w2v_empath, y_test_w2v_empath = train_test_split(X_w2v_empath, y, test_size=0.2, random_state=1)\n",
    "X_train_w2v_users_empath, X_test_w2v_users_empath, y_train_w2v_users_empath, y_test_w2v_users_empath = train_test_split(X_w2v_users_empath, y, test_size=0.2, random_state=1)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(), \n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f1adb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: Word2Vec + User\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.960     0.949     0.954     12990\n",
      "           1      0.952     0.963     0.958     13849\n",
      "\n",
      "    accuracy                          0.956     26839\n",
      "   macro avg      0.956     0.956     0.956     26839\n",
      "weighted avg      0.956     0.956     0.956     26839\n",
      "\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.842     0.845     0.843     12990\n",
      "           1      0.854     0.851     0.853     13849\n",
      "\n",
      "    accuracy                          0.848     26839\n",
      "   macro avg      0.848     0.848     0.848     26839\n",
      "weighted avg      0.848     0.848     0.848     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.768     0.801     0.784     12990\n",
      "           1      0.805     0.773     0.789     13849\n",
      "\n",
      "    accuracy                          0.786     26839\n",
      "   macro avg      0.787     0.787     0.786     26839\n",
      "weighted avg      0.787     0.786     0.787     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.811     0.834     0.822     12990\n",
      "           1      0.840     0.818     0.829     13849\n",
      "\n",
      "    accuracy                          0.826     26839\n",
      "   macro avg      0.826     0.826     0.826     26839\n",
      "weighted avg      0.826     0.826     0.826     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.895     0.872     0.883     12990\n",
      "           1      0.882     0.904     0.893     13849\n",
      "\n",
      "    accuracy                          0.888     26839\n",
      "   macro avg      0.889     0.888     0.888     26839\n",
      "weighted avg      0.888     0.888     0.888     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [15:18:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.968     0.963     0.965     12990\n",
      "           1      0.965     0.970     0.968     13849\n",
      "\n",
      "    accuracy                          0.967     26839\n",
      "   macro avg      0.967     0.966     0.966     26839\n",
      "weighted avg      0.967     0.967     0.967     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.909     0.903     0.906     12990\n",
      "           1      0.910     0.915     0.912     13849\n",
      "\n",
      "    accuracy                          0.909     26839\n",
      "   macro avg      0.909     0.909     0.909     26839\n",
      "weighted avg      0.909     0.909     0.909     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(X_train_w2v_users, X_test_w2v_users, y_train_w2v_users, y_test_w2v_users, \"Word2Vec + User\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc67863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: Word2Vec + Empath\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.959     0.948     0.954     12990\n",
      "           1      0.952     0.962     0.957     13849\n",
      "\n",
      "    accuracy                          0.955     26839\n",
      "   macro avg      0.956     0.955     0.955     26839\n",
      "weighted avg      0.955     0.955     0.955     26839\n",
      "\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.844     0.846     0.845     12990\n",
      "           1      0.855     0.854     0.854     13849\n",
      "\n",
      "    accuracy                          0.850     26839\n",
      "   macro avg      0.850     0.850     0.850     26839\n",
      "weighted avg      0.850     0.850     0.850     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.768     0.796     0.782     12990\n",
      "           1      0.802     0.774     0.788     13849\n",
      "\n",
      "    accuracy                          0.785     26839\n",
      "   macro avg      0.785     0.785     0.785     26839\n",
      "weighted avg      0.786     0.785     0.785     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.987     0.987     12990\n",
      "           1      0.988     0.988     0.988     13849\n",
      "\n",
      "    accuracy                          0.987     26839\n",
      "   macro avg      0.987     0.987     0.987     26839\n",
      "weighted avg      0.987     0.987     0.987     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.975     0.969     0.972     12990\n",
      "           1      0.971     0.977     0.974     13849\n",
      "\n",
      "    accuracy                          0.973     26839\n",
      "   macro avg      0.973     0.973     0.973     26839\n",
      "weighted avg      0.973     0.973     0.973     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [15:40:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.964     0.966     12990\n",
      "           1      0.966     0.971     0.969     13849\n",
      "\n",
      "    accuracy                          0.967     26839\n",
      "   macro avg      0.967     0.967     0.967     26839\n",
      "weighted avg      0.967     0.967     0.967     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.908     0.905     0.906     12990\n",
      "           1      0.911     0.914     0.912     13849\n",
      "\n",
      "    accuracy                          0.909     26839\n",
      "   macro avg      0.909     0.909     0.909     26839\n",
      "weighted avg      0.909     0.909     0.909     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate(X_train_w2v_empath, X_test_w2v_empath, y_train_w2v_empath, y_test_w2v_empath, \"Word2Vec + Empath\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a959ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: Word2Vec + User + Empath\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.949     0.955     12990\n",
      "           1      0.953     0.964     0.958     13849\n",
      "\n",
      "    accuracy                          0.957     26839\n",
      "   macro avg      0.957     0.957     0.957     26839\n",
      "weighted avg      0.957     0.957     0.957     26839\n",
      "\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.844     0.847     0.846     12990\n",
      "           1      0.856     0.853     0.855     13849\n",
      "\n",
      "    accuracy                          0.850     26839\n",
      "   macro avg      0.850     0.850     0.850     26839\n",
      "weighted avg      0.850     0.850     0.850     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.768     0.801     0.784     12990\n",
      "           1      0.805     0.773     0.789     13849\n",
      "\n",
      "    accuracy                          0.787     26839\n",
      "   macro avg      0.787     0.787     0.787     26839\n",
      "weighted avg      0.787     0.787     0.787     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.813     0.835     0.824     12990\n",
      "           1      0.841     0.820     0.830     13849\n",
      "\n",
      "    accuracy                          0.827     26839\n",
      "   macro avg      0.827     0.827     0.827     26839\n",
      "weighted avg      0.827     0.827     0.827     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.894     0.872     0.883     12990\n",
      "           1      0.883     0.903     0.893     13849\n",
      "\n",
      "    accuracy                          0.888     26839\n",
      "   macro avg      0.889     0.888     0.888     26839\n",
      "weighted avg      0.889     0.888     0.888     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [16:35:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.963     0.966     12990\n",
      "           1      0.965     0.971     0.968     13849\n",
      "\n",
      "    accuracy                          0.967     26839\n",
      "   macro avg      0.967     0.967     0.967     26839\n",
      "weighted avg      0.967     0.967     0.967     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.909     0.904     0.906     12990\n",
      "           1      0.910     0.915     0.912     13849\n",
      "\n",
      "    accuracy                          0.909     26839\n",
      "   macro avg      0.909     0.909     0.909     26839\n",
      "weighted avg      0.909     0.909     0.909     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate(X_train_w2v_users_empath, X_test_w2v_users_empath, y_train_w2v_users_empath, y_test_w2v_users_empath, \"Word2Vec + User + Empath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b5add4",
   "metadata": {},
   "source": [
    "# FastText + User/Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ca16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "sentences = df['tweet_tokens'].tolist()\n",
    "\n",
    "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "fasttext_model.save('fasttext_model.model')\n",
    "\n",
    "def get_vector(tokens, model):\n",
    "    word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "X_fasttext = np.array([get_vector(tokens, fasttext_model) for tokens in df['tweet_tokens']])\n",
    "X_users = df[feature_cols_users].values\n",
    "X_empath = df[feature_cols_empath].values\n",
    "\n",
    "X_fasttext_users = np.hstack([X_fasttext, X_users])\n",
    "X_fasttext_empath = np.hstack([X_fasttext, X_empath])\n",
    "X_fasttext_users_empath = np.hstack([X_fasttext, X_users, X_empath])\n",
    "\n",
    "y = df['BinaryNumTarget'].astype(int)\n",
    "\n",
    "X_train_ft_users, X_test_ft_users, y_train_ft_users, y_test_ft_users = train_test_split(X_fasttext_users, y, test_size=0.2, random_state=1)\n",
    "X_train_ft_empath, X_test_ft_empath, y_train_ft_empath, y_test_ft_empath = train_test_split(X_fasttext_empath, y, test_size=0.2, random_state=1)\n",
    "X_train_ft_users_empath, X_test_ft_users_empath, y_train_ft_users_empath, y_test_ft_users_empath = train_test_split(X_fasttext_users_empath, y, test_size=0.2, random_state=1)\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b3e2b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: FastText + User \n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.819     0.830     0.824     12990\n",
      "           1      0.838     0.828     0.833     13849\n",
      "\n",
      "    accuracy                          0.829     26839\n",
      "   macro avg      0.828     0.829     0.829     26839\n",
      "weighted avg      0.829     0.829     0.829     26839\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.930     0.936     12990\n",
      "           1      0.935     0.946     0.940     13849\n",
      "\n",
      "    accuracy                          0.938     26839\n",
      "   macro avg      0.938     0.938     0.938     26839\n",
      "weighted avg      0.938     0.938     0.938     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.790     0.815     0.802     12990\n",
      "           1      0.821     0.797     0.809     13849\n",
      "\n",
      "    accuracy                          0.806     26839\n",
      "   macro avg      0.806     0.806     0.806     26839\n",
      "weighted avg      0.806     0.806     0.806     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.804     0.764     0.783     12990\n",
      "           1      0.788     0.825     0.806     13849\n",
      "\n",
      "    accuracy                          0.795     26839\n",
      "   macro avg      0.796     0.794     0.795     26839\n",
      "weighted avg      0.796     0.795     0.795     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [17:15:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.946     0.940     0.943     12990\n",
      "           1      0.944     0.950     0.947     13849\n",
      "\n",
      "    accuracy                          0.945     26839\n",
      "   macro avg      0.945     0.945     0.945     26839\n",
      "weighted avg      0.945     0.945     0.945     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.787     0.766     0.776     12990\n",
      "           1      0.786     0.806     0.796     13849\n",
      "\n",
      "    accuracy                          0.786     26839\n",
      "   macro avg      0.786     0.786     0.786     26839\n",
      "weighted avg      0.786     0.786     0.786     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.729     0.753     0.741     12990\n",
      "           1      0.761     0.737     0.749     13849\n",
      "\n",
      "    accuracy                          0.745     26839\n",
      "   macro avg      0.745     0.745     0.745     26839\n",
      "weighted avg      0.745     0.745     0.745     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate(X_train_ft_users, X_test_ft_users, y_train_ft_users, y_test_ft_users, \"FastText + User \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99e0c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: FastText + Empath\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.821     0.833     0.827     12990\n",
      "           1      0.841     0.829     0.835     13849\n",
      "\n",
      "    accuracy                          0.831     26839\n",
      "   macro avg      0.831     0.831     0.831     26839\n",
      "weighted avg      0.831     0.831     0.831     26839\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.933     0.937     12990\n",
      "           1      0.938     0.945     0.941     13849\n",
      "\n",
      "    accuracy                          0.939     26839\n",
      "   macro avg      0.939     0.939     0.939     26839\n",
      "weighted avg      0.939     0.939     0.939     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.975     0.975     0.975     12990\n",
      "           1      0.976     0.976     0.976     13849\n",
      "\n",
      "    accuracy                          0.975     26839\n",
      "   macro avg      0.975     0.975     0.975     26839\n",
      "weighted avg      0.975     0.975     0.975     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.934     0.923     0.929     12990\n",
      "           1      0.929     0.939     0.934     13849\n",
      "\n",
      "    accuracy                          0.932     26839\n",
      "   macro avg      0.932     0.931     0.931     26839\n",
      "weighted avg      0.932     0.932     0.932     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [17:33:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.937     0.942     12990\n",
      "           1      0.942     0.951     0.946     13849\n",
      "\n",
      "    accuracy                          0.944     26839\n",
      "   macro avg      0.944     0.944     0.944     26839\n",
      "weighted avg      0.944     0.944     0.944     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.788     0.765     0.776     12990\n",
      "           1      0.785     0.807     0.796     13849\n",
      "\n",
      "    accuracy                          0.787     26839\n",
      "   macro avg      0.787     0.786     0.786     26839\n",
      "weighted avg      0.787     0.787     0.787     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.727     0.749     0.738     12990\n",
      "           1      0.758     0.736     0.747     13849\n",
      "\n",
      "    accuracy                          0.743     26839\n",
      "   macro avg      0.743     0.743     0.743     26839\n",
      "weighted avg      0.743     0.743     0.743     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate(X_train_ft_empath, X_test_ft_empath, y_train_ft_empath, y_test_ft_empath, \"FastText + Empath\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b0e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultati za: FastText + User + Empath\n",
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.818     0.828     0.823     12990\n",
      "           1      0.837     0.827     0.832     13849\n",
      "\n",
      "    accuracy                          0.828     26839\n",
      "   macro avg      0.827     0.828     0.827     26839\n",
      "weighted avg      0.828     0.828     0.828     26839\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.940     0.932     0.936     12990\n",
      "           1      0.937     0.944     0.940     13849\n",
      "\n",
      "    accuracy                          0.938     26839\n",
      "   macro avg      0.938     0.938     0.938     26839\n",
      "weighted avg      0.938     0.938     0.938     26839\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.790     0.815     0.803     12990\n",
      "           1      0.821     0.797     0.809     13849\n",
      "\n",
      "    accuracy                          0.806     26839\n",
      "   macro avg      0.806     0.806     0.806     26839\n",
      "weighted avg      0.806     0.806     0.806     26839\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.804     0.766     0.784     12990\n",
      "           1      0.790     0.825     0.807     13849\n",
      "\n",
      "    accuracy                          0.796     26839\n",
      "   macro avg      0.797     0.795     0.796     26839\n",
      "weighted avg      0.797     0.796     0.796     26839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:183: UserWarning: [18:07:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.943     0.942     0.942     12990\n",
      "           1      0.945     0.947     0.946     13849\n",
      "\n",
      "    accuracy                          0.944     26839\n",
      "   macro avg      0.944     0.944     0.944     26839\n",
      "weighted avg      0.944     0.944     0.944     26839\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.786     0.767     0.776     12990\n",
      "           1      0.786     0.805     0.795     13849\n",
      "\n",
      "    accuracy                          0.786     26839\n",
      "   macro avg      0.786     0.786     0.786     26839\n",
      "weighted avg      0.786     0.786     0.786     26839\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.731     0.753     0.742     12990\n",
      "           1      0.762     0.739     0.750     13849\n",
      "\n",
      "    accuracy                          0.746     26839\n",
      "   macro avg      0.746     0.746     0.746     26839\n",
      "weighted avg      0.747     0.746     0.746     26839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate(X_train_ft_users_empath, X_test_ft_users_empath, y_train_ft_users_empath, y_test_ft_users_empath, \"FastText + User + Empath\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
